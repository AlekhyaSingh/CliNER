\documentclass[preprint]{style}
\pagenumbering{arabic}

\usepackage{paralist}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath}
\usepackage{setspace}



% This gets rid of the block of whitespace on the first page
\makeatletter
\let\@copyrightspace\relax
\makeatother


\addtolength{\topmargin}{-.07in}
\addtolength{\textheight}{.07in}

%\addtolength{\oddsidemargin}{-0.08in}
%\addtolength{\evensidemargin}{-0.08in}
%\addtolength{\textwidth}{0.08in}
\setstretch{1.15}

\begin{document}

\title{Medical Concept Extraction}

\numberofauthors{3}
\author{
\alignauthor
Tristan Naumann\\
\affaddr{MIT EECS}
\email{tjn@mit.edu}
\alignauthor
Samantha Ainsley\\
\affaddr{MIT EECS}
\email{ainsley@mit.edu}
\alignauthor
Salman Ahmad\\
\affaddr{MIT EECS}
\email{saahmad@mit.edu}
}

\date{14 December 2012}

\maketitle
\begin{abstract}

This paper presents a system for extracting medical concepts from patient hospital records. Hospitals and medical practices are increasingly switching to electronic medical records--this trend has opened the doors to exciting applications for Natural Language Processing in medicine and biology. One such application is concept extraction: identifying high-level semantic labels in a body of text. Concept extraction on medical records is useful in automating analysis tasks that would otherwise have to be done manually. For example, hospital administrators may want to tabulate what medications are being prescribed the most. Insurance companies may want to ensure that patients receive proper care and are billed accordingly for fraud detection and auditing purposes. Government agencies may be interested in exploring and better understanding public health trends. Automatic concept extraction systems help to streamline such critical yet laborious tasks.

In service of that end, we present an algorithm and system that is capable of classifying words in medical records across four different medically-relevant categories: (1) problems, (2) tests, (3) treatments, and (4) non-medical terms. This is useful not only in granting end users the ability to analyze medical records, but also in supporting other higher-level Natural Language Processing and learning systems. This paper describes the system's design, implementation, and performance when tested on set of 500 pre-annotated medical records from Boston-area hospitals.


\end{abstract}

\section{Introduction}
TODO(SAM):
Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.

\section{Background}
TODO(SAM):
\subsection{Related Work}

\subsection{Machine Learning}

Talk about multi-class classification

\subsubsection{Support Vector Machines}

\subsubsection{Linear Regression}
Clarification: liblinear is not doing linear regression, the default is L2-regularized SVM solver using L2-loss SVC in the dual.

\subsubsection{Conditional Random Fields}

\section{Challenges}

\subsection{High Dimensionality}
TK(TJN) 

\subsection{Biased Data}
TK(TJN)

\subsection{Amount of Data}
TK(TJN)

Low amounts of train data with high amounts of tests data.

\subsection{Data Usage Agreements}
TODO(SAM):

\section{Design}

\subsection{Data Flow \& Classification Task}
Our system considers each document to be a stream of words grouped by sentences, and it attempts to provide the correct concept classification for each word. In order to identify the appropriate concept, our system considers natural language features for each document in successively broader contexts. Specifically, we first consider features using only the word itself as context, next we consider features which span the sentence, and last we consider n-gram features. In the sections that follow we discuss the linguistic features used in each context.

\subsection{Word Features}
Using only the word as context we are able to generate several interesting linguistic features. These features generally take on one of three forms: (1) intrinsic property of the word, (2) binary indicator for properties that we think are interesting, and (3) various techniques that map words into a small dimensional space such as stemming and word shape.

\subsubsection{Word}
The most obvious property of each word is perhaps the word itself. Since there is no linguistically meaningful, strict ordering over words, each word is represented as its own dimension. Consequently adding a baseline feature containing the word increases the dimension of our feature space to the size of the vocabulary in the training data. Nevertheless, the complete word serves as a fruitful feature in medical term classification in which many words carry the same meaning regardless of context. For example, terms such as "x-ray", "screen", "exam", "veal", and "sats" should be consistently recognized as test terms, just as "swelling", "failure", "trauma", and "syndrome" always indicate problems, and "therapy", "anesthesia", "vaccine", and "medication" are treatments. Granted, words like "heart" could be part of "heart rate" or "heart attack" and beg for more generic features.

\subsubsection{Length}
The next most obvious property of each word is its length. Unlike using the word as a feature, there is a strict ordering over length and consequently the addition of this feature increases the dimension of the space by only $1$. While the linguistic merit of length as a feature can be debated in conventional natural language processing, we felt it would play a large role in medical language in order to distinguish treatments, tests, and problems from words without a concept. Intuitively this is because often times treatments in the form of medications consist of long medical names, as do some of the words in medically described problems and tests.

\subsubsection{MIT Regular Expressions}
In order to capture some general characteristics of each word we included a set of regular expressions each of which adds a dimension to our classification. Such features help allow not only to pick up on traits within different medical categories--such as tests which are often abbreviated or contain numbers--but also to easily rule out non-medical terms such as days, phone numbers, etc. The characteristics we chose are as follows:
\begin{itemize}
\item {\tt INITCAP}: first letter is capitalized and alphabetic,
\item {\tt ALLCAPS}: entire word is capitalized and alphabetic,
\item {\tt CAPSMIX}: the word is a mix of capital and lowercase alphabetic characters,
\item {\tt HASDIGIT}: there is a digit in the word,
\item {\tt SINGLEDIGIT}: the token is a single digit,
\item {\tt DOUBLEDIGIT}: the token is a two digits,
\item {\tt FOURDIGITS}: the token is a series of four digits,
\item {\tt NATURALNUM}: the token is a natural number,
\item {\tt REALNUM}: the token is a real number,
\item {\tt ALPHANUM}: the token contains only digits and alphabetic characters,
\item {\tt HASDASH}: the token has a dash,
\item {\tt PUNCTUATION}: the token is punctuation,
\item {\tt PHONE1}: the token is a phone number,
\item {\tt PHONE2}: the token is a phone number with area code,
\item {\tt FIVEDIGIT}: the token is a series of five digits,
\item {\tt NOVOWELS}: the token does not contain vowels (i.e. abbreviations),
\item {\tt HASDASHNUMALPHA}: the token consists of a dash, numbers, and alphabetic characters, 
\item {\tt DATESEPERATOR}: the token is a date separator (e.g. {\tt [-/]})
\end{itemize}


\subsubsection{Porter Stemmer}
We already discussed the benefits of using entire words as features, but similar words come in many different forms. Motivated by generalization, we chose to use word stems as features and approached this feature extraction using three different stemming algorithms--two at the word-level and one at the sentence level (See 5.3.2). The addition of word stem features to our system improved our testing CRF F1 scores from 0.58 to 0.82.

Namely, we used the Porter Stemming Algorithm, as developed by Martin Porter.  This particular stemming algorithm is divided into a number of linear steps. Word letters are first replaced by symbols for vowels, V and consonants, C. The measure, m, of the word is computed as the number of repetitions of the pattern VC, and this is used to determine whether suffixes ought to be removed. Plural word forms, such as  sses $\rightarrow$ ss and removal of trailing s's, are then removed; followed by the removal of past participles; and finally terminal y's are replaced with i's. Subsequent steps handle different orderings of suffices by translating double suffices into a single suffix and then removing suffices under certain predefined conditions.

\subsubsection{Lancaster Stemmer}

Derived from the Porter Stemmer, the Lancaster Stemmer was used to provide a second word stem feature to our system. By constant, this stemming algorithm uses a table of rules that specific the removal or replacement of an ending. Endings are replaced to maintain properly-spelled words throughout the stemming process--this avoids the need for an additional recoding or partial matching phase. 

The rule table looks for the following information about each word: Endings of one or more characters (stored in reverse order); an optional intact flag; a digit specifying the removal total;  an optional append string of one or more characters;  and a continuation symbol. The stemmer first inspects the final letter of a term and considers the first relevant rule in the table. If the rule conditions are not satisfied, another rule is considered, otherwise the rule is applied and the ending is removed or reformed. This process is repeated until a termination symbol is uncovered and the word is presumed to be in its stem form.

\subsubsection{Word Shape}
In the same spirit of our word length and MIT RegEx features, we decided to look at the high-level shapes of individual words for better generalization. More specifically, we encode words into new strings that give a sense of their capitalization and punctuation patterns as well as approximate length. We tested a variety of open-sourced word shape classification methods including two methods written by Dan Klein, one of which simply classifies words as numeric, strictly upper or lower cased, or mixed, and a second, more fine-grained classifier that attempts to group words into equivalence classes by collapsing sequences of the same time. Additionally classifiers developed by Christopher Manning seek to make less distinctions mid-sequence by sorting words based on length. Each word shape classifier result added an additional dimension to our feature space. By naively including all word shape alternatives as features, we were able to improve our testbed CRF F1 score from 0.82 to 0.85. After further testing, we settled on \begin{verbatim}______\end{verbatim} as the most promising word shape metric because \begin{verbatim}______.\end{verbatim}

\subsubsection{Additional Feature Considerations}
The word-level features that we ultimately decided to include in system were chosen based on performance gain, but many others were explored in testing. These features were selected after close examination of our training data, and are worthy of discussion. It is possible, of course, that there is a more optimal feature set than our final choice that includes the features listed below:

\begin{itemize}

\item {\tt Metric Units}
A clear trend we noticed our medical record data was that numbers followed by metric weights (i.e. mg, milligrams, grams) are almost always part of treatment descriptions. Whereas numbers followed by metric lengths or areas are typically part of problem observations. We still believe this could be a useful feature for related systems.
 
\item {\tt Word Endings Indicative of Medical Problem Type}
Another obvious trend specific to medical problems are diseases ending in "itis" such as "bronchitis", "cellulites", "fibrositis", etc. and conditions ending in "ic" such as "diabetic", "anemic", etc. We added a regular expression to flag words with these endings, but the feature did little to nothing to improve performance. Such observations were likely already handled by our other word features.

\item {\tt Prognosis Locations}
Many diagnoses include locational information that should be tagged as part of the problem (i.e. "C4-C5 herniated disks"). We still think that a robust feature of this form could benefit our system, but our particular attempts proved unsuccessful.  

\item {\tt Flag Words}
Certain terms such as those described in 5.2.1 are consistently labeled as "problems", "tests", or "treatments". We attempted to create small dictionaries of these terms and assign flags to words that fall into one of the three dictionaries. The issue with this strategy is that words that words that should be labeled as one of our three medical terms but do not appear in the dictionary are now more closely aligned with purely non-medical words. That said, this feature improved accuracy for our three categories, but also yielded far too many false positives.

\end{itemize}

\subsection{Sentence Features}
Even though the primary goal of our system is to tag concepts at the word level, these individual words often need to be considered in context. Before extracting the word-level features described in 5.2, we first analyze medical records at the sentence level and assign features to our words based on their usage within those sentences.

\subsubsection{Part of Speech}
A fruitful strategy for any concept extractor is to first tag words with labels indicating their Part of Speech in context. In our case, for example, whether an ambiguous term is used as an adjective versus a noun could distinguish it as part of a test rather than a problem. We used the NTLK max entropy Part of Speech Tagger, which is trained on the Penn TreeBank to tag individual sentences. We then iterate over the returned tags and add these as word-level features, adding another dimension to our feature set for each Part of Speech tag.

\subsubsection{WordNet Lemmatizer}
Unlike the Porter Stemmer and the Lancaster Stemmer, which consider words independent of context, the WordNet Lemmatizer requires words to be processed at the sentence level in order to extract their root form. The WordNet Lemmatizer uses the part of speech tags for individual words, which are assign via NLTK as previously mentioned, in conjunction with a subset of tags used for morphology to extract stems from each word. WordNet references a database of word stems, and uses two types of processes to try to convert input strings into a form present in the database. A list of inflectional endings based on the word's syntactic tag is referenced in search of a match that can be detached from the word. An exception list is also referenced in search for an inflected form. WordNet first checks for exceptions and then uses rules for detachment to transform the word--after each transformation the result is referenced against the stem database until a match is eventually found.

\subsubsection{Formatted Text}
When reading through our training data, we noticed that test results are written in a variety of distinct forms that clearly separate these terms from problem or treatment terms. Since test result forms often spent multiple words such was "O2 sat -- 98\%" or "screen was positive". We wrote a series of regular expressions to capture these patterns with the test term as the first term in the sequence. We iterate over each word in the sentence and run our RegEx suite on the inclusive righthand sequence starting from that word. This feature adds only one additional dimension, and improved our CRF F1 test score from .85 to .90 by boosting our test term detection appreciably.

\subsection{N-gram Features}
TODO(SAM):

\subsubsection{Previous}

\subsubsection{Next}

\subsection{Parameter Tuning}

\begin{figure}
\begin{center}
	\includegraphics[width=1\columnwidth]{figures/parameter-selection.png}
\end{center}
\caption{An illustration of the grid search process to identify the best parameters values for training the SVM model.}
\label{fig:parameter_selection}
\end{figure}

\begin{itemize}

\item Grid search and grid search image

\item Include that PNG image from easy.py

\item Cross validation

\end{itemize}




\section{System}

\subsection{Code Architecture}

\begin{figure*}
\begin{center}
	\includegraphics[width=2\columnwidth]{figures/system.pdf}
\end{center}
\caption{An overview of the system and codebase.}
\label{fig:system}
\end{figure*}


We built a fairly comprehensive framework for applying machine learning techniques to NLP domains. We built out a series of useful and reusable library modules and command line utilities to perform various tasks. An overview of the codebase can be seen in Figure~\ref{fig:system} and  all of the code is available online at the following URL:

{\tt https://github.com/tnaumann/ConceptExtraction}.

\subsubsection{Data Formats and File I/O}

Our framework consists of many convenience functions and utilities classes that are designed to perform all necessary file I/O operations formatting output data and parsing input files. There are 3 formats that the code base handles. 

The first format is a straight forward tokenized text file that contains each sentence on a separate line. The test data for training the models were processed into this format. For cases where the sentence was not previously tokenized, the framework utilizes {\tt nltk}'s {\tt sent\_tokenize}  to break a string into sentences and then {\tt nltk}'s {\tt word\_tokenize} to break each sentence into normalized tokens.

The second format is an annotation format that references the tokenized text files and associates a classification label (``problem'', ``tests'', ``treatment'' or ``none'') with spans of words. This format is used in a separate file than the actual input tokenized text file but the two files typically share the same name so we can tell which labels apply to which input. The training and test data sets use this format to represent the ground truth classification of the medical records.

The third and last format is is a sparse matrix format that is utilized by the various command-line utilizes in the code base. Conceptually, the rows in this sparse matrix represent an a single data item (in our case, a single word) that either is a training example or is an input token that needs to a label predicted and the columns represent the various features of that item. The various learning algorithms all use this data format.

\subsubsection{Model Encapsulation and Serialization}

Our framework includes a model class that can be trained on certain data with certain features enabled. The model class supports training a libSVM, libLinear, and CRF classifier that can be used for prediction in the future. One of the nice things, about the model class is that it can be serialized and reloaded dynamically. This is a really nice capability because it allows you to train a large number of models on various subsets of the data and using various different features to see which perform the best.

\subsubsection{Feature Computation}

The framework has a flexible and robust mechanism for computing features on input tokens. Features are implemented as a simple conditional statement in a large {\tt if-else} statement, making it really easy to add new features to the system. Additionally, each model maintains a list of the features that are enabled. Only the enabled features will be executed and computed on the input tokens. Features can be enabled or disabled pragmatically by removing them from the model's feature list. This ability is really important when training models because it enables you to easily explore the impact of certain features and run feature selection.

\subsubsection{Learning Algorithms}

As mentioned previously, the framework's model class has support for training various classifiers: a Support Vector Machine, a CRF, and linear classifier. The implementation of these algorithms is delegates to various third-party libraries described below.

\subsubsection{Data Organization and Directory Structure}


The framework supports a default conventional directory structure to know where to store test, training, and prediction data. This simplifies training new models and testing how well they perform. While these directories can be configured, the defaults are:

\begin{itemize}

\item {\tt data } -  Contains all of the data that is used for training and large testing runs. Again, this is just conventional and it is possible to train and predict on data that is not in this directory. For example, we are able to predict ``one-off-sentences'' as a command-line argument.

\item {\tt data/<data-set-name>/txt } - Contains the raws tokenized text files.

\item {\tt data/<data-set-name>/concept } - Contains the labels that correspond to each of the tokenized files in the ``txt'' directory.

\item {\tt models} - Contains all of the trained models.

\item {\tt models/<model-name>} - Contains the meta data specific to each model that was trained. The framework saves the model's internal state (the features used, the input test set, etc.) as well as the trained parameters for the libSVM, crfsuite, and libLinear classifiers. This information is used to deserialize and load a model at runtime for prediction tasks.

\item {\tt models/<model-name>/predictions/} - Contains output files with the predicted label for each word in the input file.

\end{itemize}

\subsubsection{Command-line Utilities}


The framework contains three useful command line utilities to interact with the system. The functionality provided by these utilities are also exposed by a Python API so other developers can interact with the system natively in their own Python scripts without having to fork calls to the command line. 

\begin{itemize}

\item {\tt train.py } -  Trains a model using the specified training data and output model name.

\item {\tt predict.py } -  Predict the labels for the specified input files and write the labels to a designated file.

\item {\tt evaluate.py } -  Compare the predicted labels to the gold standard labels and print outs the confusion matrix, precision, recall, and F1-scores. 

\end{itemize}


\subsection{Web Service}

In addition to the various command line utilizes described in the previous section our system also includes a Web-based service that allows users to manually input a single sentence, select a model to use for prediction, and see the predicted labels that model generated for the input. Figure~\ref{fig:web_interface} shows a screenshot of the web application in use.

The Web service was written in Python and directly interoperates with the rest of the code base. To run the Web applications simply execute the {\tt web.py} file in the source tree. The Web app allows you to label a sentence using any of the models that we have previously trained. This is useful for spot checking certain sentences and observing how different models behave. The Web app assumes that models have already been trained and placed into ``models'' directory in the source tree. By default, the application will host itself on port 5000. You can also see a live version of the Web service at:

{\tt http://saahmad.media.mit.edu:5000/ }

\begin{figure}[t]
\begin{center}
	\includegraphics[width=0.75\columnwidth]{figures/web-interface-1.png}
	\includegraphics[width=0.75\columnwidth]{figures/web-interface-2.png}
\end{center}
\caption{The Web application interface that allows users to manually input a sentence and have it labeled using one of the previously-trained models. In this case we are using an SVM model that was trained using all of the features.}
\label{fig:web_interface}
\end{figure}


\subsection{Libraries}

Our code leveraged various 3rd party libraries for training the models and generating features. A summary of the libraries that we used is shown in the list below:

\begin{itemize}

\item {\tt libsvm} \cite{libsvm}: libsvm provides a fast implementation of Support Vector Machines. It includes a command line utility that takes a training data (with all of the features formatted in a sparse matrix representation) as input and outputs a SVM model trained on that data. It also includes a command line utility that allows users to specify a desired model and predict the classes on new input. Our code base forks process calls to these utilities to train SVM models, although a lot of the parameter tuning and feature selection is handled elsewhere in our codebase.

\item {\tt liblinear} \cite{liblinear}: lib linear is a linear classifier library that has special support multi-class classification. It also provides easy to use training and predicting command line utilities that accept a similar input format as libsvm.

\item {\tt crfsuite} \cite{crfsuite}: crfsuite is an implementation for conditional random fields and includes command line front ends to train a model on data and to predict the labels on an unseen data set. The data format for the input training files is also a sparse matrix representation but is slightly different from the format used by libsvm and lib linear. Our codebase nicely abstracts out these format differences and produces the right format depending on the model in use.

\item {\tt nltk} \cite{nltk}: nltk is a very popular NLP library written in Python. We leverage nltk to generate some of the features on our data. In particular, we utilize nltk's maximum-entropy part-of-speech tagging as well as many of its stemming algorithms.

\item {\tt flask} \cite{flask}: Flask is a micro Web framework written in Python. It contains a small yet useful set of features that allow developers to easily setup RESTful Web backends and an elegant tempting system to gracefully generate HTML. Flask is used to build the Web service portion of the project.

\end{itemize}
	

\section{Results}

\subsection{Data Set}

The data set that we used train and test the system came from the i2b2 NLP challenge. As mentioned before, the data was already divided into test and training sets and every word was pre-labeled with the following classes: ``problem'', ``test'', ``treatment'', or ``none''. 

The training data consisted of 170 medical records and the test data consisted of 256 medical records.  The overall size of the data was 10mb. The data all came from Boston-area hospitals and is subject to a data usage agreement.

\subsection{Trained Models}

Overall we trained a total of 23 models using the entire data set. One model was trained with all of the features. We then trained a set of 11 models in which we removed a single feature as well as 11 models in which we had only one feature enabled. Even though this is not as complete as a running full feature selection, it will hopefully hint at the most useful features.

\subsection{Runtime Performance}

We deployed the system on a machine running Ubuntu 11.10 with a QuadCore AMD Opteron processor running a 2.5Ghz and 48GB of ram.

In general training the liblinear and crfsuite classifiers was pretty quick. Each took around an hour to train on the entire training set. However, the libsvm classifier took approximate 6 hours to train on the same data.

In terms of predicting labels for unseen input, all classifiers performed equally well. They each took around 1 hour to label the entire test data set. However, the vast major of time was spent computing features on the input rather than actually running the classifiers.

\subsection{Classification Performance}

A table of a bunch of awesome results. We should present results by model type (SVM, CRF, LIN) and feature set.

\section{Discussion}

Talk about what we saw in terms of which was the best feature set and model. 

Be sure to include examples of cool cases where it caught a difficult label

Be sure to include FAILURE cases


We stopped using libSVMs.


\section{Conclusion and Future Work}


Found that libSVM was not worth while to use

\subsection{Dimension Collapse}

\subsection{Feature Selection}

\subsection{Parameter Selection}

\subsection{More Data}

It goes without saying that having more data is always nice. This is particularly true in our case because we were training on a relatively small data set and testing on a much larger one. In fact, an interesting avenue for future work is to switch our training and testing data sets so we are training on relatively larger data set.

\section{Acknowledgments}

We sincerely thank Dr. Robert Berwick  and Geza Kovacs
for their guidance, help, and support.


%%%% May the Flow (Max-Flow, that is) be with you all.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{citations}  % sigproc.bib is the name of the Bibliography in this case

\balancecolumns
% That's all folks!
\end{document}
